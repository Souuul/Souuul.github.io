I"<h2 id="linear-regression-model">Linear Regression Model</h2>

<p>Linear Regression을 함수로 표현하기전에 경사하강법에 대하여 알아보도록 하겠습니다.</p>

<p>학습데이터를 관통하는 하나의 직선이 존재한다는 Hypothesis를 만들어 낼 수 있으며 그 식은 <code class="language-plaintext highlighter-rouge">H(x) = Wx +b</code>라고 표현하였습니다. 그렇다면 최적의 Hypothesis를 만들기 위해서는 loss function 혹은 cost function 의  최소값을 구해야합니다.하지만 이것을 구하기 쉽지 않기때문에 우리는 경사하강법을 이용하여 최소값에 근사한 값을 구해보도록 하겠습니다.</p>

<h4 id="경사하강법">경사하강법</h4>

<p>아래 그림대로 손실함수 그래프를 따라가면서 손실함수가 최소가 되는 지점에서의 W를 구하는 것입니다. W가 변화하면서 Hypothesis는 오른쪽 아래의 그림처럼 변화하게 됩니다.</p>

<p align="center"><img src="../../assets/image/image-20200922223143462.png" alt="image-20200922223143462" /></p>

<p>이것을 수식으로 표현하면 하기와 같습니다.</p>

<p><img src="../../assets/image/image-20200922230346991.png" alt="image-20200922230346991" style="zoom:20%;" /></p>

<p>경사하강법의 다음 도착점은 W와 α(Learning rate) 그리고 손실함수의 미분값곱의 차로 결정됩니다. 여기서 Learning rate 값이 너무 크거나 작으면 하기 그림과 같은 상황이 벌어지게 됩니다.</p>

<p align="center"><img src="../../assets/image/image-20200922230706564.png" alt="image-20200922230706564" style="zoom:50%;" /></p>

<p>그렇기 때문에 적절한 α을 찾아서 배정해주는 것이 중요합니다.</p>

:ET