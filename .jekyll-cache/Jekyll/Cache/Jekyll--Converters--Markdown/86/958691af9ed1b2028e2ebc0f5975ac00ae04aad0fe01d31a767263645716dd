I"RA<h2 id="linear-regression-model">Linear Regression Model</h2>

<p>Linear Regression을 함수로 표현하기전에 경사하강법에 대하여 알아보도록 하겠습니다.</p>

<p>학습데이터를 관통하는 하나의 직선이 존재한다는 Hypothesis를 만들어 낼 수 있으며 그 식은 <code class="language-plaintext highlighter-rouge">H(x) = Wx +b</code>라고 표현하였습니다. 그렇다면 최적의 Hypothesis를 만들기 위해서는 loss function 혹은 cost function 의  최소값을 구해야합니다.하지만 이것을 구하기 쉽지 않기때문에 우리는 경사하강법을 이용하여 최소값에 근사한 값을 구해보도록 하겠습니다.</p>

<h4 id="경사하강법">경사하강법</h4>

<p>아래 그림대로 손실함수 그래프를 따라가면서 손실함수가 최소가 되는 지점에서의 W를 구하는 것입니다. W가 변화하면서 Hypothesis는 오른쪽 아래의 그림처럼 변화하게 됩니다.</p>

<p align="center"><img src="../../assets/image/image-20200922223143462.png" alt="image-20200922223143462" /></p>

<p>이것을 수식으로 표현하면 하기와 같습니다.</p>

<p align="center"><img src="../../assets/image/image-20200922230346991.png" alt="image-20200922230346991" style="zoom:20%;" /></p>

<p>경사하강법의 다음 도착점은 W와 α(Learning rate) 그리고 손실함수의 미분값곱의 차로 결정됩니다. 여기서 Learning rate 값이 너무 크거나 작으면 하기 그림과 같은 상황이 벌어지게 됩니다.</p>

<p align="center"><img src="../../assets/image/image-20200922230706564.png" alt="image-20200922230706564" style="zoom:50%;" /></p>

<p>그렇기 때문에 적절한 α을 찾아서 배정해주는 것이 중요합니다.</p>

<p>그렇다면 이번에는 코드로서 표현을 해보도록 하겠습니다. 학습데이터는 이전시간에 사용하였던 공부시간과 시험점수에 대한 데이터를 동일하게 사용하도록 하겠습니다.</p>

<p>코드를 구현하는 절차는 하기와 같습니다.</p>

<blockquote>
  <ol>
    <li>
      <p>Training Data Set을 준비
 머신러닝에 입력으로 사용될 데이터를 NumPy array(ndarray)형태로 준비합니다.</p>
    </li>
    <li>
      <p>Linear Regression Model을 정의
 y = Wx + b =&gt; model을 프로그램적으로 표현합니다. W 와 b에 대한 변수 선언한 후 초기값은 랜덤값을 이용합니다.</p>
    </li>
    <li>
      <p>loss function을 정의</p>

      <p>손실함수(loss function)에 대한 코드를 작성 후 matrix처리해야해요</p>
    </li>
    <li>
      <p>learning rate의 정의</p>

      <p>일반적으로 customizing이 되는 값으로 초기에는 0.001정도로 설정해서 사용하고 loss값을 보고 수치를 조절할 필요가 있습니다.</p>
    </li>
    <li>
      <p>학습을 진행</p>

      <p>반복적으로 편미분을 이용해서 W와 b를 update하는 방식으로 구현</p>
    </li>
  </ol>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s">'공부시간(x)'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">13</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">18</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">30</span><span class="p">],</span>
       <span class="s">'시험점수(t)'</span><span class="p">:[</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">31</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span><span class="mi">46</span><span class="p">,</span><span class="mi">49</span><span class="p">,</span><span class="mi">60</span><span class="p">,</span><span class="mi">62</span><span class="p">,</span><span class="mi">70</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">85</span><span class="p">,</span><span class="mi">91</span><span class="p">,</span><span class="mi">92</span><span class="p">,</span><span class="mi">97</span><span class="p">,</span><span class="mi">98</span><span class="p">]}</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">13</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">18</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">30</span><span class="p">]).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">t_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">31</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span><span class="mi">46</span><span class="p">,</span><span class="mi">49</span><span class="p">,</span><span class="mi">60</span><span class="p">,</span><span class="mi">62</span><span class="p">,</span><span class="mi">70</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">85</span><span class="p">,</span><span class="mi">91</span><span class="p">,</span><span class="mi">92</span><span class="p">,</span><span class="mi">97</span><span class="p">,</span><span class="mi">98</span><span class="p">]).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#데이터의 분포를 scatter로 확인
</span>
<span class="c1"># plt.scatter(x_data.ravel(), y_data.ravel()) # ravel() 무조건 1차원으로 변경
# plt.show()
</span>
<span class="c1"># Linear Regression Model을 정의
# y = Wx+b
</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># matrix
</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># scalar
</span>
<span class="c1"># H = W*x + b # y대신에 Hypothesis 를 나타내는 H를 변수명으로 썼어요!
</span>
<span class="c1"># loss function 
</span><span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">t</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">W</span><span class="p">)</span><span class="o">+</span><span class="n">b</span> 
    
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">((</span><span class="n">t</span><span class="o">-</span><span class="n">y</span><span class="p">),</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># (t-y)^2/n
</span>
<span class="c1"># 미분함수 (중앙차분)
</span><span class="k">def</span> <span class="nf">numerical_derivative</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">delta_x</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">derivative_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s">'multi_index'</span><span class="p">])</span>
    
    <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="p">.</span><span class="n">finished</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">it</span><span class="p">.</span><span class="n">multi_index</span>
        
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">=</span> <span class="n">tmp</span> <span class="o">+</span> <span class="n">delta_x</span>
        <span class="n">fx_plus_delta</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">=</span> <span class="n">tmp</span> <span class="o">-</span> <span class="n">delta_x</span>
        <span class="n">fx_minus_delta</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">derivative_x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fx_plus_delta</span> <span class="o">-</span> <span class="n">fx_minus_delta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">delta_x</span><span class="p">)</span>
        
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span>
        
        <span class="n">it</span><span class="p">.</span><span class="n">iternext</span><span class="p">()</span>
        
    <span class="k">return</span> <span class="n">derivative_x</span>

<span class="c1"># prediction
</span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">W</span><span class="p">)</span><span class="o">+</span><span class="n">b</span> <span class="c1"># Hypthesis, Linear Regression Model
</span>
<span class="c1"># learning rate라는 상수가 필요, 정의해야해요!
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0001</span>

<span class="c1"># 미분을 진행할 loss_func에 대한 lambda 함수를 정의
</span><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">t_data</span><span class="p">)</span>

<span class="c1"># 학습을 진행!!
# 반복해서 학습을 진행 ( W 와 b를 update하면서 반복적으로 학습을 진행)
</span><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">90000</span><span class="p">):</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">numerical_derivative</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="c1"># W의 편미분
</span>    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">numerical_derivative</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="c1"># W의 편미분
</span>    
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">3000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'W : {}, b : {}, loss : {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">loss_func</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">t_data</span><span class="p">)))</span>

<span class="c1"># 학습 종료 후
</span>
<span class="k">print</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="mi">19</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">t_data</span><span class="p">.</span><span class="n">ravel</span><span class="p">())</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_data</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span><span class="n">W</span><span class="p">)</span><span class="o">+</span><span class="n">b</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p align="center"><img src="../../assets/image/image-20200922231343703.png" alt="image-20200922231343703" style="zoom:50%;" /></p>

:ET