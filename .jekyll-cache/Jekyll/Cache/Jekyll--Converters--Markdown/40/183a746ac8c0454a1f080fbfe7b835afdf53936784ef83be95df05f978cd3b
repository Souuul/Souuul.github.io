I"<h2 id="linear-regression-model">Linear Regression Model</h2>

<p>Linear Regression을 함수로 표현하기전에 경사하강법에 대하여 알아보도록 하겠습니다.</p>

<p>학습데이터를 관통하는 하나의 직선이 존재한다는 Hypothesis를 만들어 낼 수 있으며 그 식은 <code class="language-plaintext highlighter-rouge">H(x) = Wx +b</code>라고 표현하였습니다. 그렇다면 최적의 Hypothesis를 만들기 위해서는 loss function 혹은 cost function 의  최소값을 구해야합니다.하지만 이것을 구하기 쉽지 않기때문에 우리는 경사하강법을 이용하여 최소값에 근사한 값을 구해보도록 하겠습니다.</p>

<h4 id="경사하강법">경사하강법</h4>

<p>아래 그림대로 손실함수 그래프를 따라가면서 손실함수가 최소가 되는 지점에서의 W를 구하는 것입니다. W가 변화하면서 Hypothesis는 오른쪽 아래의 그림처럼 변화하게 됩니다.</p>

<p align="center"><img src="../../assets/image/image-20200922223143462.png" alt="image-20200922223143462" /></p>

<p>이것을 수식으로 표현하면 하기와 같습니다.</p>

<p><img src="../../assets/image/image-20200922230346991.png" alt="image-20200922230346991" style="zoom:20%;" /></p>

<p>경사하강법의 다음 도착점은 W와 α(Learning rate) 그리고 손실함수의 미분값곱의 차로 결정됩니다. 여기서 Learning rate 값이 너무 크거나 작으면 하기 그림과 같은 상황이 벌어지게 됩니다.</p>

<p align="center"><img src="../../assets/image/image-20200922230706564.png" alt="image-20200922230706564" style="zoom:50%;" /></p>

<p>그렇기 때문에 적절한 α을 찾아서 배정해주는 것이 중요합니다.</p>

<p>그렇다면 이번에는 코드로서 표현을 해보도록 하겠습니다. 학습데이터는 이전시간에 사용하였던 공부시간과 시험점수에 대한 데이터를 동일하게 사용하도록 하겠습니다.</p>

<ol>
  <li>
    <p>Training Data Set을 준비
머신러닝에 입력으로 사용될 데이터를 NumPy array(ndarray)형태로 준비!</p>
  </li>
  <li>
    <p>Linear Regression Model을 정의
y = Wx + b =&gt; model을 프로그램적으로 표현
W 와 b에 대한 변수 선언한 후 초기값은 랜덤값을 이용할 꺼에요!!</p>
  </li>
  <li>
    <p>loss function을 정의 : 손실함수(loss function)에 대한 코드를 작성
matrix처리해야해요!!</p>
  </li>
  <li>
    <p>learning rate의 정의 : 일반적으로 customizing이 되는 값으로 초기에는 
0.001정도로 설정해서 사용하고 loss값을 보고 수치를 조절할 필요가 있어요!!</p>
  </li>
  <li>
    <p>학습을 진행 : 반복적으로 편미분을 이용해서 W와 b를 update하는 방식으로 구현</p>
  </li>
</ol>
:ET